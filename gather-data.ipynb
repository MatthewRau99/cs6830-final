{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Chat Generated Articles from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Set api key here, left empty for account privacy reasons. You can add you own api key if you want to try\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"C:\\\\Users\\\\graduate\\\\Box\\\\Courses taken\\\\CS 6830 Data Science in practice\\\\Final project\\\\openai_api.txt\", 'r') as f:\n",
    "#     api_key = f.read().strip()\n",
    "\n",
    "# # Set api key here, left empty for account privacy reasons. You can add you own api key if you want to try\n",
    "# openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles is a list of dictionaries. Each dictionary contains all the information needed to generate one news article.\n",
    "# Information is gotten by scraping existing articles online\n",
    "# Currently, the prompt requires the the new organization: 'source', wordcount the generated article should be: 'wordcount'\n",
    "# and the headline: 'headline'. An example is shown below\n",
    "articles = [{'source':'CNN', 'wordcount':'200', 'headline':'What is the Good Friday Agreement? How a historic deal ended the Troubles in Northern Ireland'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFakeArticles(articles):\n",
    "    fakeArticles = pd.DataFrame(columns=['Headline', 'Content'])\n",
    "    for article in articles:\n",
    "        prompt=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are a journalist at {article['source']}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Write an article with about {article['wordcount']} words with the headline '{article['headline']}'\"},\n",
    "        ]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=prompt\n",
    "        )\n",
    "        fakeArticles.loc[len(fakeArticles.index)] = [article['headline'], response['choices'][0]['message']['content']]\n",
    "        print(f'Articles created: {len(fakeArticles)}')\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "    return fakeArticles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make CNN Fake Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('CNN500_items.csv')\n",
    "\n",
    "# ai_50 = df.sample(frac = 0.5)\n",
    "# real_50 = df.drop(ai_50.index)\n",
    "\n",
    "# articles = []\n",
    "# for index, row in ai_50.iterrows():\n",
    "#     articles.append({'source':'CNN','wordcount':len(row.Description.replace('\\n','').split(' ')),'headline':row.Title.replace('\\n','').strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_gen = getFakeArticles(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_gen.columns = ['Title', 'Content']\n",
    "# ai_gen['Source'] = 1\n",
    "\n",
    "# non_ai = real_50[['Title', 'Description']]\n",
    "# non_ai.columns = ['Title', 'Content']\n",
    "# non_ai['Source'] = 0\n",
    "\n",
    "# cnn_mixed = pd.concat([ai_gen, non_ai], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_mixed.to_csv(path_or_buf=\"cnn_mixed.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make AP Fake Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('AP501dataset.csv')\n",
    "\n",
    "# df.loc[0].text\n",
    "# ai_50 = df.sample(frac = 0.5)\n",
    "# real_50 = df.drop(ai_50.index)\n",
    "\n",
    "# articles = []\n",
    "# for index, row in ai_50.iterrows():\n",
    "#     articles.append({'source':'AP News','wordcount':len(row.text.replace('\\n','').split(' ')),'headline':row.title.replace('\\n','').strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_gen = getFakeArticles(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_gen.columns = ['Title', 'Content']\n",
    "# ai_gen['Source'] = 1\n",
    "\n",
    "# non_ai = real_50[['title', 'text']]\n",
    "# non_ai.columns = ['Title', 'Content']\n",
    "# non_ai['Source'] = 0\n",
    "\n",
    "# ap_mixed = pd.concat([ai_gen, non_ai], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ap_mixed.to_csv(\"ap_mixed.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Fake Fox News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('fox260.csv')\n",
    "\n",
    "# ai_50 = df.sample(frac = 0.5)\n",
    "# real_50 = df.drop(ai_50.index)\n",
    "\n",
    "# articles = []\n",
    "# for index, row in ai_50.iterrows():\n",
    "#     articles.append({'source':'Fox News','wordcount':len(row.text.replace('\\n','').split(' ')),'headline':row.title.replace('\\n','').strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_gen = getFakeArticles(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_gen.columns = ['Title', 'Content']\n",
    "# ai_gen['Source'] = 1\n",
    "\n",
    "# non_ai = real_50[['title', 'text']]\n",
    "# non_ai.columns = ['Title', 'Content']\n",
    "# non_ai['Source'] = 0\n",
    "\n",
    "# fox_mixed = pd.concat([ai_gen, non_ai], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fox_mixed.to_csv(\"fox_mixed.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Megh's Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the three CSV files into separate DataFrames\n",
    "ap_mixed1 = pd.read_csv('ap_mixed.csv')\n",
    "display(ap_mixed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_mixed1 = pd.read_csv('cnn_mixed.csv')\n",
    "display(cnn_mixed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_mixed1 = pd.read_csv('fox_mixed.csv')\n",
    "display(fox_mixed1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merging 3 datasets into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the three DataFrames vertically (row-wise)\n",
    "dataset = pd.concat([ap_mixed1, cnn_mixed1, fox_mixed1], ignore_index=True).reset_index()\n",
    "dataset[750:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optionally, you can drop duplicates if needed\n",
    "# dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "dataset['Content'].replace('', np.nan, inplace=True)\n",
    "dataset['Content'] = dataset.Content.apply(lambda x : x.strip())\n",
    "dataset['Title'] = dataset.Title.apply(lambda x : x.strip())\n",
    "\n",
    "dataset = dataset[dataset.Title != \"Toxic smoke is spewing from an inferno at a recycling plant known as a 'fire hazard,' officials say. The flames could burn for days\"].reset_index()\n",
    "\n",
    "\n",
    "#drop NA values\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Select only specific columns from the dataset\n",
    "selected_columns = ['Title', 'Content', 'Source']\n",
    "dataset_final = dataset[selected_columns]\n",
    "\n",
    "display(dataset_final)\n",
    "#final_dataset = dataset_final.to_csv('final_dataset.csv')\n",
    "dataset_final.shape\n",
    "# dataset_final.info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "countplot of the real and fake news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=dataset,x='Source')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the Content column in all lower case words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert 'Content' column to lowercase\n",
    "dataset_final['L_Content'] = dataset_final['Content'].str.lower()\n",
    "dataset_final.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove stpwords and punctuation from the content of our news combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuations=string.punctuation\n",
    "dataset_final['L_Content'] = dataset_final.L_Content.apply(lambda x: ' '.join([word for word in x.split() \n",
    "                    if word not in (stop) and word[0] != '@' \n",
    "                    and word not in punctuations ]))\n",
    "dataset_final.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final['L_Content'] = dataset_final['L_Content'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "dataset_final.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize the news content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NLTK tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "# Create an instance of the PorterStemmer class\n",
    "porter_stemmer = PorterStemmer()\n",
    "# Replace null values with an empty string\n",
    "dataset_final['L_Content'].fillna('', inplace=True)\n",
    "# Tokenize the 'Content' column only for non-null values\n",
    "dataset_final['L_Content'] = dataset_final['L_Content'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for stemming\n",
    "def stem_words(tokens):\n",
    "    return [porter_stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Apply stemming to the 'Content' column\n",
    "dataset_final['L_Content'] = dataset_final['L_Content'].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a matrix with every unique word in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the list of words into a single string\n",
    "dataset_final['L_Content'] = dataset_final['L_Content'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "# Convert the 'Content' column to lowercase\n",
    "dataset_final['L_Content'] = dataset_final['L_Content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,1), min_df=10)\n",
    "vectorized_tweets = cv.fit_transform(dataset_final['L_Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test and training dataset\n",
    "X=vectorized_tweets\n",
    "y=dataset_final['Source'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes classification produces good results when we use it for textual data analysis such as Natural Language Processing\n",
    "Gaussian NB: When we have continuous attribute values, we made an assumption that the values associated with each class are distributed according to Gaussian or Normal distribution\n",
    "Multinominal NB: Multinomial Naïve Bayes algorithm is preferred to use on data that is multinomially distributed. It is one of the standard algorithms which is used in text categorization classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit data to a multinomial Naive Bayes model\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "#display(y_pred)\n",
    "p,r,f,s = precision_recall_fscore_support(y_test, y_pred)\n",
    "print(p, r, f, s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy claculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred_train = clf.predict(X_train)\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for overfitting and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the scores on training and test set\n",
    "print('Training set score: {:.4f}'.format(clf.score(X_train, y_train)))\n",
    "print('Test set score: {:.4f}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "array = [[tp, fp],[fn, tn]]\n",
    "sns.heatmap(array, annot=True, annot_kws={\"size\": 16}, fmt='d', cmap='YlGnBu', vmin=0, vmax=140, cbar=True, linewidths=0.5, linecolor='k', square=True)\n",
    "sns.despine(left=False, right=False, top=False, bottom=False)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "# plt.savefig('1x1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_article_sorted = clf.feature_log_prob_[0, :].argsort()[::-1]\n",
    "ai_article_sorted = clf.feature_log_prob_[1, :].argsort()[::-1]\n",
    "print('Orig article words:\\n', np.take(cv.get_feature_names_out(), orig_article_sorted[:50]))\n",
    "print('\\nFake article words:\\n', np.take(cv.get_feature_names_out(), ai_article_sorted[:50]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
